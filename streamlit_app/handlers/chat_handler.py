from langchain_groq import ChatGroq
import streamlit as st
import time
from typing import List, Dict, Any, Optional
import os
import sys
import tiktoken
from langchain.chains import RetrievalQA

# Check if running on Streamlit Cloud
if "mnt" in os.getcwd():
    os.chdir("/mount/src/linkedin-chatbot-job-mnt-team/")
    sys.path.append("/mount/src/linkedin-chatbot-job-mnt-team/")

from streamlit_app.config.config import Config
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# Initialize configuration
config = Config()
api_key = st.secrets["GROQ_API_KEY"]

# Initialize configuration
config.initialize_session_states()

# variables
time_sleep_var = config.get_config()["time_sleep"]
temperature_var = config.get_config()["temperature"]
model_name_var = config.get_config()["model_name"]
max_tokens_var = int(config.get_config()["max_tokens"])
assistant_message_row = int(config.get_config()["assistant_message_row"])
defaul_model_token = config.get_config()["defaul_model_token"]
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"


class ChatHandler:
    def __init__(self, model_name: str = model_name_var, temperature: int = temperature_var):
        self.llm = ChatGroq(api_key=api_key, model_name=model_name, temperature=temperature, max_tokens=max_tokens_var)
        self.time_sleep = time_sleep_var
        self.max_tokens = max_tokens_var
        self.encoding = tiktoken.get_encoding(defaul_model_token)
        # Initialize vector store components
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            cache_folder="streamlit_app/models/vector_db/" # Cache embeddings locally
        )
        self.vector_db = self._load_vector_store()
        self.retriever = self.vector_db.as_retriever(search_kwargs={"k": 3})

    def _load_vector_store(self) -> Optional[FAISS]:
        """Initialize and load FAISS vector store"""
        try:
            vector_db_path = config.get_config()["vector_db_path"]
            index_faiss_path = os.path.join(vector_db_path, "index.faiss")
            index_pkl_path = os.path.join(vector_db_path, "index.pkl")

            # Kiểm tra xem cả hai file có tồn tại không
            if not os.path.exists(index_faiss_path) or not os.path.exists(index_pkl_path):
                print("Vector database files not found! Please generate them first.")
                return None
            
            print(f"Loading FAISS index from {vector_db_path}...")

            vector_db = FAISS.load_local(
                folder_path=vector_db_path,
                embeddings=self.embeddings,
                allow_dangerous_deserialization=True
            )
            return vector_db
        except Exception as e:
            print(f"Error loading FAISS: {e}")
            return None
        

    def retrieve_qa(self, query) -> str:
        """
        Use the retriever to run a question-answering chain based on retrieved documents.

        Args:
            vector_db: The vector database containing embedded documents.
            llm: The language model for generating answers.

        Returns:
            str: The response generated by the LLM.
        """
        try:
            if not self.vector_db or not self.retriever:
                return "Vector database not initialized properly"

            qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=self.retriever,
                return_source_documents=True
            )
            
            response = qa_chain.invoke(query)
            return response["result"]
        except Exception as e:
            st.error(f"Error in retrieve_qa: {str(e)}")
            return None
    
    def filter_messages(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """
        Filter messages from chat history, placing user messages at the end
        
        Args:
            messages (List[Dict[str, str]]): List of message dictionaries
        
        Returns:
            List[Dict[str, str]]: Filtered messages with user messages at the end
        """
        try:
            # Get assistant messages first
            result_filter = []
            assistant_messages = [msg for msg in messages if msg["role"] == "assistant"]
            user_messages = [msg for msg in messages if msg["role"] == "user"]
            
            # Add last N assistant messages if they exist
            if assistant_messages:
                result_filter.extend(assistant_messages[-assistant_message_row:])
            
            # Add user messages at the end
            result_filter.extend(user_messages)
            
            return result_filter
        except Exception as e:
            st.error(f"Error filtering messages: {str(e)}")
            return []
    
    def count_tokens(self, text: str) -> int:
        """
        Count the number of tokens in a message
        
        Args:
            message (str): Input message
        
        Returns:
            int: Number of tokens
        """
        return len(self.encoding.encode(text))

    def check_token_limit(self, messages: List[Dict[str, str]]) -> bool:
        """
        Check if the combined messages exceed token limit
        
        Args:
            messages (List[Dict[str, str]]): List of message dictionaries
            
        Returns:
            bool: True if within limit, False if exceeds
        """
        total_tokens = sum(self.count_tokens(msg["content"]) for msg in messages)

        return total_tokens <= self.max_tokens
    
    def generate_response(self, messages: List[Dict[str, str]]) -> str:
        """
        Generate response using the chat model
        
        Args:
            messages (List[Dict[str, str]]): List of message dictionaries with 'role' and 'content'
        
        Returns:
            str: Generated response
        """
        try:
            if not self.check_token_limit(messages):
                st.error(f"Message length exceeds token limit of {self.max_tokens}")
                return None
            
            response = self.llm.invoke(
                input=messages,
                max_tokens=self.max_tokens
            )
            return response.content
        except Exception as e:
            st.error(f"Error generating response: {str(e)}")
            return None
    
    def stream_response(self, response: str, placeholder: Any) -> None:
        """
        Stream the response with typing effect
        
        Args:
            response (str): Response text to stream
            placeholder: Streamlit placeholder for displaying response
        """
        full_response = ""
        for chunk in response.split():
            full_response += chunk + " "
            time.sleep(self.time_sleep)
            placeholder.markdown(full_response + "▌")
        placeholder.markdown(full_response)
    
    def format_messages_for_model(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """
        Format messages for model input
        
        Args:
            messages (List[Dict[str, str]]): Raw messages from chat history
        
        Returns:
            List[Dict[str, str]]: Formatted messages for model
        """
        return [
            {
                "role": msg["role"],
                "content": msg["content"]
            }
            for msg in messages
        ]

    def validate_message(self, message: str) -> bool:
        """
        Validate user message
        
        Args:
            message (str): User input message
        
        Returns:
            bool: True if valid, False otherwise
        """
        if not message or not message.strip():
            return False
        return True

    @staticmethod
    def create_message(role: str, content: str) -> Dict[str, str]:
        """
        Create a message dictionary
        
        Args:
            role (str): Message role (user/assistant)
            content (str): Message content
        
        Returns:
            Dict[str, str]: Formatted message dictionary
        """
        return {
            "role": role,
            "content": content
        }