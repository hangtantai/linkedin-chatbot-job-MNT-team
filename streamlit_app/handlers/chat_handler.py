from langchain_groq import ChatGroq
import streamlit as st
import time
from typing import List, Dict, Any, Optional
import os
import sys
import tiktoken
from langchain.chains import RetrievalQA
import threading

# Check if running on Streamlit Cloud
if "mnt" in os.getcwd():
    os.chdir("/mount/src/linkedin-chatbot-job-mnt-team/")
    sys.path.append("/mount/src/linkedin-chatbot-job-mnt-team/")

from streamlit_app.utils.config import Config
from streamlit_app.utils.utils_chat import check_token_limit
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# Initialize configuration
config = Config()
api_key = st.secrets["GROQ_API_KEY"]

# Initialize configuration
config.initialize_session_states()

# Variables
time_sleep_var = config.get_config()["time_sleep"]
temperature_var = config.get_config()["temperature"]
model_name_var = config.get_config()["model_name"]
max_tokens_var = int(config.get_config()["max_tokens"])
assistant_message_row = int(config.get_config()["assistant_message_row"])
defaul_model_token = config.get_config()["defaul_model_token"]
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"

class ChatHandler:
    _embeddings = None
    _vector_db = None
    _is_initialized = False
    _initialization_lock = threading.Lock()

    def __init__(self, model_name: str = model_name_var, temperature: int = temperature_var):
        self.llm = ChatGroq(api_key=api_key, model_name=model_name, temperature=temperature, max_tokens=max_tokens_var)
        self.time_sleep = time_sleep_var
        self.max_tokens = max_tokens_var
        self.encoding = tiktoken.get_encoding(defaul_model_token)

        # Set initialization flags
        self.is_ready = False
        
        # Start background initialization
        threading.Thread(target=self._initialize_in_background, daemon=True).start()

    def _initialize_in_background(self):
        """Initialize embeddings and vector database in a background thread"""
        try:
            with ChatHandler._initialization_lock:
                if not ChatHandler._is_initialized:
                    # Load embeddings
                    if ChatHandler._embeddings is None:
                        print("Loading embeddings model...")
                        ChatHandler._embeddings = HuggingFaceEmbeddings(
                            model_name="sentence-transformers/all-MiniLM-L6-v2",
                            cache_folder="streamlit_app/models/vector_db/"
                        )
                    
                    # Load vector store
                    if ChatHandler._vector_db is None:
                        print("Loading vector database...")
                        ChatHandler._vector_db = self._load_vector_store()
                    
                    ChatHandler._is_initialized = True
                    print("ChatHandler initialization complete!")
                
                # Set instance variables
                self.embeddings = ChatHandler._embeddings
                self.vector_db = ChatHandler._vector_db
                self.retriever = None if self.vector_db is None else self.vector_db.as_retriever(search_kwargs={"k": 3})
                self.is_ready = True
        except Exception as e:
            print(f"Error during initialization: {e}")
            self.is_ready = True

    def _load_vector_store(self) -> Optional[FAISS]:
        """Initialize and load FAISS vector store"""
        try:
            vector_db_path = config.get_config()["vector_db_path"]
            index_faiss_path = os.path.join(vector_db_path, "index.faiss")
            index_pkl_path = os.path.join(vector_db_path, "index.pkl")

            # check 2 files exist
            if not os.path.exists(index_faiss_path) or not os.path.exists(index_pkl_path):
                print("Vector database files not found! Please generate them first.")
                return None
            
            print(f"Loading FAISS index from {vector_db_path}...")

            vector_db = FAISS.load_local(
                folder_path=vector_db_path,
                embeddings=self._embeddings,
                allow_dangerous_deserialization=True
            )
            return vector_db
        except Exception as e:
            print(f"Error loading FAISS: {e}")
            return None
        
    def retrieve_qa(self, query) -> str:
        """
        Use the retriever to run a question-answering chain based on retrieved documents.

        Args:
            vector_db: The vector database containing embedded documents.
            llm: The language model for generating answers.

        Returns:
            str: The response generated by the LLM.
        """
        try:
            if not self.vector_db or not self.retriever:
                return "Vector database not initialized properly"

            qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=self.retriever,
                return_source_documents=True
            )
            
            response = qa_chain.invoke(query)
            return response["result"]
        except Exception as e:
            st.error(f"Error in retrieve_qa: {str(e)}")
            return None
    
    def filter_messages(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """
        Filter messages from chat history, placing user messages at the end
        
        Args:
            messages (List[Dict[str, str]]): List of message dictionaries
        
        Returns:
            List[Dict[str, str]]: Filtered messages with user messages at the end
        """
        try:
            # Get assistant messages first
            result_filter = []
            assistant_messages = [msg for msg in messages if msg["role"] == "assistant"]
            user_messages = [msg for msg in messages if msg["role"] == "user"]
            
            # Add last N assistant messages if they exist
            if assistant_messages:
                result_filter.extend(assistant_messages[-assistant_message_row:])
            
            # Add user messages at the end
            result_filter.extend(user_messages)
            
            return result_filter
        except Exception as e:
            st.error(f"Error filtering messages: {str(e)}")
            return []
    
    def generate_response(self, messages: List[Dict[str, str]]) -> str:
        """
        Generate response using the chat model
        
        Args:
            messages (List[Dict[str, str]]): List of message dictionaries with 'role' and 'content'
        
        Returns:
            str: Generated response
        """
        try:
            if not check_token_limit(max_tokens_var, self.encoding, messages):
                st.error(f"Message length exceeds token limit of {self.max_tokens}")
                return None
            
            response = self.llm.invoke(
                input=messages,
                max_tokens=self.max_tokens
            )
            return response.content
        except Exception as e:
            st.error(f"Error generating response: {str(e)}")
            return None
    
    def stream_response(self, response: str, placeholder: Any) -> None:
        """
        Stream the response with typing effect
        
        Args:
            response (str): Response text to stream
            placeholder: Streamlit placeholder for displaying response
        """
        full_response = ""
        for chunk in response.split():
            full_response += chunk + " "
            time.sleep(self.time_sleep)
            placeholder.markdown(full_response + "â–Œ")
        placeholder.markdown(full_response)